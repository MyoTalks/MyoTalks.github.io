<!DOCTYPE HTML>
<html>
  <head>
    <!-- Google analytics tag (gtag.js) (currently disabled)
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-STGLQW4BJX"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-STGLQW4BJX');
    </script>
    -->

    <!-- Title -->
    <title>MyoTalks</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=1000">

    <!-- Isotope JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.13.2/jquery-ui.min.js"></script>
    <script src="https://unpkg.com/isotope-layout@3/dist/isotope.pkgd.min.js"></script>

    <!-- Custom Style -->
    <link rel="stylesheet" href="style.css">

    <!-- Google Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
    <style>
      @import url('https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap');
    </style>
  </head>

  <body id="body">

    <div id="main">
      <div id="intro">
        <div id="intro-text">
          <h1>MyoTalks</h1>
          <p>
            MyoTalks is an online podcast series organized by the <a href="https://sites.google.com/view/myosuite">MyoSuite</a> team to invite scientists and researchers from robotics, biomechanics, machine learning, neuroscience, sports sciences, rehabilitation and clinical sciences, human-computer interaction, ergonomics, and any related fields to share insights and works towards understanding full-scale, end-to-end human embodied intelligence. <a href="https://join.slack.com/t/myosuite/shared_invite/zt-1zkpw2zzk-NhVhVlSDxhoMHbzROD8gMA">Slack Channel</a> and <a href="https://calendar.google.com/calendar/u/0?cid=NmY1YTQ0M2I5MDM2ZjUzZGRlYzY2OWU5M2JiYjZmM2E2MGNjMjRhZTcyZjhmZjJhOTk3N2QxOTIwMGJhNzcwOEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t">Calendar</a>
          </p>
        </div>
        <div id="intro-image">
          <img src="/profile.jpeg">
        </div>
      </div>

      <div class="image-quote-container" style="text-align: center;">
    <!-- Image in the center -->
    <img src="/material_3.png" alt="Description of Image" style="width: 300px; height: auto; border-radius: 8px; padding-top: 30px;">

    <!-- Quote at the bottom -->
    <blockquote style="margin-top: 20px; font-size: 1.5rem; font-style: italic; color: #555;padding-bottom: 20px;">
        "What I cannot create, I do not understand" - Richard Feynman
    </blockquote>
</div>


      <div id="filters" class="button-group">
        <!-- <button class="button" data-filter="*">Show All</button> -->
        <button class="button is-checked" data-filter=".highlight">Upcoming Event</button>
        <button class="button" data-filter=".publication">Past Events</button>
      </div>

      <div class="grid">
        
<!-- Events -->     

<div id="highlights-2" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Biomechanical User Simulations for HCI</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            April 17, 2025 8:00 AM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            <a href="https://fl0fischer.github.io/">Florian Fischer (University of Cambridge, Human-Computer Interaction)</a>
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/florian.png" alt="Florian Fischer" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            In Human-Computer Interaction (HCI), interfaces and interaction techniques are typically validated and compared through application-specific user studies and well-established quantitative models that enable the prediction of summary statistics, e.g., the time it takes to complete a given task. Inspired by recent advances in neighbouring fields, we propose forward simulation of biomechanical user models as a complementary, highly powerful tool for HCI that can provide novel insights into how and why users move during interaction, as well as the interdependencies between user perception and control. In combination with state-of-the-art optimization and ML methods, this approach allows researchers and designers to predict movement trajectories and ergonomic variables, such as fatigue, prior to conducting user studies. In this talk, I will discuss the potential of biomechanical simulation for HCI and interface optimisation, as well as current limitations and challenges. In addition, I will present SIM2VR, a system that uses RL methods to simulate how users interact with a given VR application. This system, for the first time, enables training simulated users directly in the same VR application that real users interact with, which represents a major step towards automated biomechanical testing in XR.        </p>
      </i>  
      <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=_T1fhYAv6zE&t=269s">Recording</a></p>
    </div>
</div>

<div id="highlights-2" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Using Embodied AI to help answer "why" questions in systems neuroscience</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            April 25, 2025 2:30 â€“ 3:30PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            <a href="https://anayebi.github.io/">Aran Nayebi (Carnegie Mellon University, Machine Learning & Neuroscience)</a>
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/aran.jpeg" alt="Aran Nayebi" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
  Deep neural networks trained on high-variation tasks ("goals") have had immense success as predictive models of the human and non-human primate visual pathways. More specifically, a positive relationship has been observed between model performance on ImageNet categorization and neural predictivity. Past a point, however, improved categorization performance on ImageNet does not yield improved neural predictivity, even between very different architectures. In this talk, I will present two case studies in both rodents and primates, that demonstrate a more general correspondence between self-supervised learning of visual representations relevant to high-dimensional embodied control and increased gains in neural predictivity. In the first study, we develop the (currently) most precise model of the mouse visual system, and show that self-supervised, contrastive algorithms outperform supervised approaches in capturing neural response variance across visual areas. By "implanting" these visual networks into a biomechanically-realistic rodent body to navigate to rewards in a novel maze environment, we observe that the artificial rodent with a contrastively-optimized visual system is able to obtain more reward across episodes compared to its supervised counterpart. The second case study examines mental simulations in primates, wherein we show that self-supervised video foundation models that predict the future state of their environment in latent spaces that can support a wide range of sensorimotor tasks, align most closely with human error patterns and macaque frontal cortex neural dynamics. Taken together, our findings suggest that representations that are reusable for downstream embodied tasks may be a promising way forward to study the evolutionary constraints of neural circuits in multiple species.
            </i>  
      <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=CdaLJXAvsj4&t=2851s">Recording</a></p>
    </div>
</div>

<div id="highlights-patrick" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Towards Personalizing Assistive Technology in the Real-world</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Jul. 2, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Patrick Slade (Harvard University, Bioengineering)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/patrick.png" alt="Patrick Slade" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            Hundreds of millions of people face mobility. Wearable sensing and assistive devices offer the potential of helping people overcome these challenges, but determining how to improve mobility or health metrics may be unclear. We will discuss several case studies involving developing and personalizing assistive technology for real-world use: a wearable system for tracking calories burned during exercise, a portable exoskeleton that personalizes assistance during real-world walking, and a navigation aid for people with impaired vision. These projects focus on developing technology that is low-cost and easily reproducible to work towards tools for underserved populations.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=0ADt_llN9GQ&t=1s">Recording</a></p>
    </div>
</div>

<div id="highlights-alpaslan" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Wireless and Programmable Neurostimulator System for In Vivo Neural Microstimulation</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Jul. 16, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Alpaslan ErsÃ¶z (Carnegie Mellon University, Mechanical Engineering)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/alpaslan.png" alt="Alpaslan ErsÃ¶z" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            This talk presents the design and validation of a wireless, programmable, multi-channel neurostimulator system for in vivo neural microstimulation. The system integrates discrete analog circuitry and embedded firmware to enable charge-balanced current stimulation, programmable anodic biasing for enhanced charge injection, and voltage transient monitoring to ensure electrochemical safety. It also incorporates artifact suppression for concurrent neural recording. The device's compact design supports multiple stimulation modalities including amplitude and frequency modulation. Validation was performed through benchtop, in vitro, and in vivo experiments, demonstrating up to a ten-fold increase in charge injection capacity and successful artifact-free recording. The talk will highlight the system architecture, performance results, and implications for microstimulation in animal studies.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=P4VB9H5tPa4&t=5s">Recording</a></p>
    </div>
</div>

<div id="highlights-steven" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Neural mechanisms of motivated movement</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Jul. 23, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Steven Chase (Carnegie Mellon University, Biomedical Engineering & Neuroscience Institute)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/steven.png" alt="Steven Chase" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            Movements are influenced by motivation. Consider a basketball player shooting a free throw. Depending on the stakes of the outcome of the shot, performance can vary greatly. Top athletes rise to the challenge, and perform better during a game than they do during practice. But when the stakes are inordinately high, like when the game is on the line, even skilled players can "choke under pressure", and under-perform right when it matters the most. Here I will explore the neural mechanisms that link motivation to changes in movement, by investigating how neural population activity in primary motor cortex changes as a function of reward. We find clear neural signatures of reward in motor cortex that can predict, on a trial-by-trial basis, whether choking under pressure is likely.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=FTwGRu28gww">Recording</a></p>
    </div>
</div> 

<div id="highlights-Park" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">MyoChallenge Round-Table 1: Achieving State-of-the-art Musculoskeletal Control</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Wed. October 2nd, 2:00 AM - 3:00 AM Eastern Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
           <a href="https://scholar.google.com/citations?user=Cv5lSo0AAAAJ&hl=it">Alberto Chiappa</a> and <a href="https://sites.google.com/mrl.snu.ac.kr/jungnam/home">Jungnam Park</a>
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/material_4.jpeg" alt="Alberto Chiappa" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
            <img src="/material_5.jpg" alt="Jungnam Park" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            In this week's MyoTalks, we are excited to welcome past MyoChallenge winnersâ€”Alberto Chiappa and Jungnam Parkâ€”to share their experiences and strategies for excelling in the NeurIPS competition track. Achieving biological dexterity remains a key objective in robotics, biomechanics, and neural control of movement. Insights from a variety of scientific disciplines are essential to advance state-of-the-art techniques, and we are thrilled to host the MyoChallenge Round-table to explore the knowledge our winners have contributed from their respective fields toward advancing musculoskeletal control and helping them achieving state-of-the-art.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://youtu.be/uQ2QZznae8M?si=7n4uSYRxIQROegmA">Recording</a></p>
    </div>
</div>

<div id="highlights-Wang" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">MyoSuite/MyoChallenge: Towards Full-Scale Human Embodied Intelligence</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Wed., Sep 11, 2024 10:00 AM - 11:00 AM EDT
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Vittorio Caggiano, Chun Kwang Tan, Cheryl Wang
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/material_11.png" alt="Speaker 1" style="width: 150px !important; height: 150px !important; border-radius: 50% !important; object-fit: cover !important; border: 2px solid #ddd !important; margin-right: 10px !important;">
            <img src="/material_12.png" alt="Speaker 2" style="width: 150px !important; height: 150px !important; border-radius: 50% !important; object-fit: cover !important; border: 2px solid #ddd !important; margin-right: 10px !important;">
            <img src="/material_13.png" alt="Speaker 3" style="width: 150px !important; height: 150px !important; border-radius: 50% !important; object-fit: cover !important; border: 2px solid #ddd !important;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
                Abstract: Humans are embodied intelligent beings acting in the physical world. Achieving and understanding such intelligence has been the holy grail for neuroscience, AI, and robotics communities. Nevertheless, a full-scale end-to-end level of understanding has been unattainable for a long time due to the problem's complexity. Researchers in neuroscience, AI, and robotics each turn into simpler sub-problems in their fields. Here, we introduce MyoSuite, an embodied AI platform that simulates human intelligence, end-to-end and full-scale, by integrating machine learning, biomechanical muscle models, and neural control of movement. This platform enables the generation of physiologically realistic movements, such as dexterous manipulation, which holds significant potential for applications in prosthetics, rehabilitation, neuroscience, and building human androids. We will also introduce SAR and MyDex, our state-of-the-art models for human in-hand dexterity and MyoChallenge-24, this year's NeurIPS competition track for building the best embodied human models.
            </i><br>
            Together with Theoretical and Computational Neuroscience Journal Club at Johns Hopkins University
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://youtu.be/uQ2QZznae8M?si=7n4uSYRxIQROegmA">Recording</a></p>
    </div>
</div>

<div id="highlights-doug" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="highlight">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Sensing and stimulating the brain to restore neurological function</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Aug. 6, 2025 16:00 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Doug Weber (Carnegie Mellon University, Mechanical Engineering & Neuroscience)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/doug.png" alt="Doug Weber" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            Significant advances in materials and microelectronics over the last decade have enabled clinically relevant technologies that measure and regulate neural signaling in the brain, spinal cord, and peripheral nerves. These technologies provide new capabilities for studying basic mechanisms of information processing and control in the nervous system, while also creating new opportunities for restoring function lost to injury or disease. Neural sensors can also measure the activity of motor neurons to enable direct neural control over prosthetic limbs and assistive technologies. Conversely, these neural interface technologies can stimulate activity in sensory and motor neurons to reanimate paralyzed muscles. Although many of these applications rely currently on devices that must be implanted into the body for precise targeting, ultra-miniaturized devices can be injected through the skin or vascular system to access deep structures without open surgery. This talk will focus on efforts to develop wearable and injectable neural interfaces for restoring or improving motor function in people with paralysis due to stroke, spinal cord injury, ALS, and other neurological disorders.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=mxOqXibjBL8">Recording</a></p>
    </div>
</div>

<div id="highlights-chenhao" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="highlight">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Learning from Demonstrations: from Generative Adversarial Training to Representation Learning</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Sept. 17, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Chenhao Li (ETH Zurich, AI Center)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/chenhao.png" alt="Chenhao Li" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            This talk explores recent advances in learning from demonstrations (LfD), with a focus on motion priors and policy learning for robotics and embodied agents. We examine two primary methodological streams: generative adversarial training and feature-based representation learning. We compare the two methods and discuss the key challenges they present, including issues such as discriminator saturation, mode collapse, limited or noisy data, and sparse supervision. To address these challenges, we present a series of algorithmic innovations, including Wasserstein-based adversarial frameworks, constrained style mimicry, mutual information maximization, latent manifold representations via frequency-domain parameterization, and automatic reference generation. These techniques enable more robust, data-efficient learning and allow policies to generalize beyond the original demonstration data. Finally, we highlight the connections and differences between the two approaches, and how they can be leveraged together to advance motion learning in complex environments.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://youtu.be/FZ7XYUMjglQ">Recording</a></p>
    </div>
</div>

<div id="highlights-nidhi" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="highlight">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Predictive Principles of Motor Behavior</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Oct. 1, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Nidhi Seethapathi (MIT, Brain and Cognitive Sciences & EECS)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/nidhi.png" alt="Nidhi Seethapathi" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            The best current robots still fall short of the efficiency and safety guarantees exhibited by biological systems. One way to understand this superior performance is to develop computational models that predict how animals select, execute, and learn everyday movements. Despite this need, most of our current computational and theoretical understanding is limited to simple tasks or explanatory models with limited predictive breadth. My talk will highlight the predictive principles of safe and efficient motor behavior we've uncovered recently: the cost functions, controller structures, and learning rules. These principles will provide a blueprint for engineering human-like performance in wearable and autonomous robots.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=am1bGdgzpf4">Youtube</a></p>
    </div>
</div>

<div id="highlights-steve" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item highlight" data-category="highlight">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Learning, Hierarchies, and Reduced Order Models</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Oct. 8, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Steve Heim (Cornell University, Research Scientist)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/steve.png" alt="Steve Heim" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            With the advent of ever more powerful compute and learning pipelines that offer robust end-to-end performance, are hierarchical control frameworks with different levels of abstraction still useful? Hierarchical frameworks with reduced-order models (ROMs) have been commonplace in model-based control for robots, primarily to make long-horizon reasoning computationally tractable. I will discuss some of the other advantages of hierarchies, why we want ROMs and not simply latent spaces, and the importance of matching the time scale to each level of the hierarchy. In particular, I will show some results in learning for legged robots using ROMs with cyclic inductive bias, with both hand-designed and data-driven ROMs. I will also discuss using viability measures to estimate the intuitive notion of "how confident/safe is this action" and why this is only useful at the right level of abstraction.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://cmu.zoom.us/j/93728692493">Zoom Link</a></p>
    </div>
</div>
 <!--       
  <div id="highlights-maria" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item highlight" data-category="highlight">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Disturbance detection during locomotion and effective assistance for balance recovery in aging gait</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Oct. 15, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Maria Tagliaferri (Carnegie Mellon University, Mechanical Engineering)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/maria.png" alt="Maria Tagliaferri" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            Falls during daily ambulation are a leading cause of injury among older adults, often resulting from delayed physiological responses to balance disturbances such as slips and trips. Lower-limb exoskeletons hold promise for reducing fall risk by detecting and responding to these perturbations faster than the human user. However, a critical first step toward effective exoskeleton-based balance support is the development of real-time, onboard methods for perturbation detection. While whole-body angular momentum (WBAM) is a commonly used metric, it is suboptimal for exoskeleton applications due to its high computational demands and reliance on extensive parameter tuning. To address these limitations, our group is developing a novel perturbation detection framework based on lower-limb kinematics during walking. In parallel, we aim to bridge key knowledge gaps regarding when and how assistance should be applied to effectively enhance the user's balance recovery. Using a single-degree-of-freedom hip exoskeleton device developed in the lab, we are investigating human responses to a range of sagittal-plane perturbations to inform the design of control strategies that augment balance without interfering with natural movement. Specifically, we are implementing and evaluating both biomechanical model-based and neural network-based control architectures to understand their effect on recovery time, muscle activation, and metabolic cost in response to perturbations.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://cmu.zoom.us/j/93728692493">Zoom Link</a></p>
    </div>
</div>
--> 
  <div id="highlights-youngjoong" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item highlight" data-category="highlight">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Making 3D Human Digitization Affordable, Efficient, and Accessible</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Oct. 17, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            YoungJoong Kwon (Emory University, Computer Science)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/youngjoong.png" alt="YoungJoong Kwon" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            Human digitizationâ€”a process that digitally captures a subject's appearance, expressions, and movementsâ€”holds tremendous promise for transcending physical barriers and improving lives. Yet despite its potential, the technology remains largely inaccessible due to costly studio setups and reliance on specialized expertise. In this talk, I will discuss how these limitations can be addressed by presenting affordable, efficient, and user-friendly approaches to human digitization. First, I will focus on reconstruction from sparse observations, leveraging 3D priors and temporal information to compensate for limited camera inputs and reliably capture geometry even under occlusions. Second, I will introduce an efficient representation that reduces computational demands, using light fields for fast, high-quality synthesis. Finally, I will propose easy-to-interact representations that eliminate complex pipelines: by integrating generative models, a single reference image and minimal user input can drive the creation of novel poses and views without extensive test-time optimization. Lastly, I will explore potential applications of these approaches, highlighting how they might further improve lives through more affordable, efficient, and accessible human digitization solutions.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://cmu.zoom.us/j/93728692493">Zoom Link</a></p>
    </div>
</div>

<div id="highlights-john" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Mapping representations for motor control and awareness in the cerebral cortex</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Mar. 22, 2025 14:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            <a href="https://psychology.uchicago.edu/directory/john-veillette">John Veillette, (University of Chicago, Neuroscience)</a>
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/john.png" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
              Our bodies are the fundamental input-output interface between the brain and world, and the conscious experience of acting through this interface, the sense of agency (SoA), is among the most basic facets of self-awareness. A critical challenge in the cognitive sciences is to understand which types of motor control representations percolate into awareness as SoA â€“ and where SoA may deviate from a veridical metric of control. In this talk, I will discuss research in which we manipulate participants' experience of agency while usurping control of their muscles using functional electrical stimulation. We find evidence that this experience of volition over the musculature can be decoded from human brain recordings. We then present a proof-of-concept study in which we use the representations of a simulated biomechanical controller (trained in MyoSuite) to human functional magnetic resonance imaging recordings during a motor task, and we find that neural activity in brain regions predicted by the MyoSuite model can also be used to "decode" SoA days later. We close by discussing challenges and opportunities for using neuromuscular control models to understand human brain activity in controlled and in naturalistic tasks, with a particular focus on the potential of personalized brain models.
        </i>
              <p style="font-size: 16px !important; text-align: center !important;"><a href="https://www.youtube.com/watch?v=ACmtIk83hf8">Recording</a></p>
    </div>
</div>   
        
<div id="highlights-Schumacher" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item publication" data-category="publication">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">MyoChallenge Round-Table 2: Achieving State-of-the-art Musculoskeletal Control</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Oct 17, 2024 10:00 AM Eastern Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
          Kaibo He and Pierre Schumacher
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/pierre.png" alt="Pierre Schumacher" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
            <img src="/kaibo.png" alt="Kaibo He" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            Kaibo He and Pierre Schumacher will give a 15-minute presentation on their research, scientific interests, and work. Another 15-minute presentation will cover how they designed their solution, including their approach to the challenge, the rationale behind their design, and why they believe their solution worked best. Finally, they should spend about 10 minutes discussing what motivated them to join the challenge and how it has advanced their own work.            </i>
        </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://youtu.be/uQ2QZznae8M?si=7n4uSYRxIQROegmA">Recording</a></p>
    </div>
</div>


        <div id="highlights-nikhil" style="padding: 10px !important; max-width: 300px !important; margin: 10px !important; font-family: Arial, sans-serif !important;">
    <div class="list-item highlight" data-category="highlight">
        <p style="margin: 20px 0 !important; font-size: 18px !important; font-weight: bold !important; padding-top: 30px !important; text-align: left !important;">Sensing & Modulation of Neural Activity for Motor Restoration and Enhanced Assistive Interaction</p>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            Oct. 29, 2025 13:30 PM Eastern Standard Time
        </p>
        <p style="margin: 20px 0 !important; font-size: 16px !important; text-align: center !important;">
            Nikhil Verma (Carnegie Mellon University, Mechanical Engineering)
        </p>
              <div class="speaker-image" style="margin-top: 20px !important; text-align: center !important;">
            <img src="/nikhil.png" alt="Nikhil Verma" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover; border: 2px solid #ddd;">
        </div>
        <p style="margin: 10px 0 !important; font-size: 16px !important; line-height: 1.6 !important; text-align: left !important;">
            <i>
            Neurological conditions like stroke and spinal cord injury damages the corticospinal tract and disrupts the communication between the brain and body, leading to paralysis. Addressing these deficits, particularly hand function in individuals with tetraplegia, is a critical clinical priority. In this talk, I will discuss complementary strategies in neural sensing and neuromodulation designed to address these motor deficits and enhance assistive interactions. Using high-density electromyography (HDEMG), we demonstrate that motor unit activity can be detected even in muscles clinically diagnosed as paralyzed. These subtle myoelectric signals can be decoded, providing possible control signals for assistive devices for individuals with severe motor paralysis. Complementing this sensing approach, electrical stimulation of the spinal cord delivered below the injury site can recruit spinal sensorimotor circuits that remain intact after the injury. Our studies highlight how both non-invasive (transcutaneous) and invasive (epidural) spinal stimulation effectively restore voluntary motor function after paralysis from stroke or spinal cord injury. Taken together, recent advances in sensing and modulating neural activity form a comprehensive and synergistic approach to restore motor function and improve interaction capabilities for individuals living with chronic paralysis.
            </i>
        </p>
        <p style="font-size: 16px !important; text-align: center !important;"><a href="https://cmu.zoom.us/j/93728692493">Zoom Link</a></p>
    </div>
</div>

<!-- Footner -->
        
</div>
      <div id="footer">Inspired by <a href="https://github.com/andyzeng/andyzeng.github.io">Andy Zeng's website</a>.</div>
</div>

    <script>

      // Isotope grid.
      var $grid = $('.grid').isotope({
        itemSelector: '.list-item',
        layoutMode: 'fitRows',
        transitionDuration: 0,
        stagger: 10,
        initLayout: false,
        getSortData: {
          name: '.name',
          symbol: '.symbol',
          number: '.number parseInt',
          category: '[data-category]',
          weight: function( itemElem ) {
            var weight = $( itemElem ).find('.weight').text();
            return parseFloat( weight.replace( /[\(\)]/g, '') );
          }
        }
      });

      // Bind filter button click.
      $('#filters').on( 'click', 'button', function() {
        var filterValue = $( this ).attr('data-filter');
        localStorage.setItem('filterValue', filterValue);
        $grid.isotope({ filter: filterValue });
      });

      // Change is-checked class on buttons.
      $('.button-group').each( function( i, buttonGroup ) {
        var $buttonGroup = $( buttonGroup );
        $buttonGroup.on( 'click', 'button', function() {
          $buttonGroup.find('.is-checked').removeClass('is-checked');
          $( this ).addClass('is-checked');
        });
      });

      function update_isotope() {
        // Retrieve cached button click.
        var defaultFilterValue = localStorage.getItem('filterValue');
        if (defaultFilterValue == null) {
          defaultFilterValue = ".highlight"
        }
        $grid.isotope({ filter: defaultFilterValue });
        var buttons = document.getElementsByClassName("button");
        for (var currButton of buttons) {
          if (currButton.getAttribute('data-filter') == defaultFilterValue) {
            currButton.classList.add('is-checked');
          } else {
            currButton.classList.remove('is-checked');
          }
        }
      }

      function toggle_bio() {
        var x = document.getElementById("more-bio");
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }

      function toggle_highlights() {
        var x = document.getElementById("main-highlights");
        var y = document.getElementById("more-highlights");
        var b = document.getElementById("toggle_highlights_button")
        if (y.style.display === "none") {
          x.style.display = "none";
          y.style.display = "block";
          b.innerHTML = "Show less"
          update_isotope();
        } else {
          x.style.display = "block";
          y.style.display = "none";
          b.innerHTML = "Show more"
          update_isotope();
        }
      }

      update_isotope();

    </script>
    </div>
  </body>
</html>
